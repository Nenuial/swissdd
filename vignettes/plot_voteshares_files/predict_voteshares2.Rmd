---
title: "Predict vote shares"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{test}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
#<img class="plain"  src="https://swissvotes.ch/static/images/logo.svg"/>
#usethis::use_vignette("tolle_beispiele", "geiles zeug mit dem swissdd packe tun")
```

## How to get Swissvotes Data

First, you have to make sure that your machine is connected to the internet. The function `get_swissvotes` fetches all data stored within the swissvotes database. Swissvotes offers one of the most comprehensive data platforms in relation to Swiss referendums and initiatives.

![](https://swissvotes.ch/static/images/logo.svg)


```{r echo=TRUE, warning=FALSE, message=FALSE}

# installation from CRAN (stable)
# install.packages("swissdd")
# install.packages("dplyr")

# installation from github (ongoing updates)
# devtools::install_github("politanch/swissdd")

library(swissdd)
library(dplyr)
library(ggplot2)
```


By default, the function simply extracts the database provided by [Swissvotes](https://swissvotes.ch){target="_blank"}. However, you can specify that you want the codebook as well (or just the codebook for that matter). Additionally, there is a specification to save the citation. If you work with data from Swissvotes, please cite the data accordingly.

We first extract the data and the select the variables we need to make predictions. For some reason we may believe that the National Council represents the people. Therefore, we hope to see a pattern between the final votes in the National Council and the actual yes shares on a voting day.

We download and prettify the data. After that we calculate the share of yes votes in the National Council and create a scatterplot to inspect if our intuition about the relationship can be seen in the data.

```{r echo=TRUE, warning=FALSE, message=FALSE, eval=FALSE}
#extract data from database
swissvotesDB <- get_swissvotes(DB=T, savecitation=F, codebook=F) %>%
  #select variables we need
  dplyr::select(anr, volkja.proz, nrja, nrnein, anzahl, rechtsform, br.pos, srja, srnein, p.fdp, p.cvp, p.sps,p.svp,p.evp,p.gps,p.sd,p.edu, w.fdp, w.cvp, w.sp, w.svp, w.evp, w.gps,
                p.kvp, p.glp, p.bdp) %>%
  #make numeric and rename variables
  mutate(nrja = as.numeric(nrja),
         nrnein = as.numeric(nrnein),
         srja = as.numeric(srja),
         srnein = as.numeric(srnein),
         volkja.proz= volkja.proz/100)%>%
  rename(bfsnr=anr, 
         yesshare = volkja.proz)%>%
  filter(bfsnr<6310)%>%
  mutate_at(vars(contains("p.")), list(~ifelse(.==9999, NA, .)))

#clean data
swissvotesDB[swissvotesDB=="."] <- NA

#calculate share of yes votes in national council
swissvotesDB <- swissvotesDB %>%
  mutate(nr_yesshare = nrja / (nrja+nrnein),
         sr_yesshare = srja / (srja+srnein))

swissvotesDB1 <- swissvotesDB %>% na.omit %>%
  mutate_at(vars(contains("p.")), ~recode(., `1`=1, `2`=-1, `5`=0, `6`=0))%>%
  mutate(rechtsform=as.factor(rechtsform))%>%
  mutate_at(vars(contains("w.")), (~as.numeric(.)/100))

umweg <- swissvotesDB1%>%dplyr::select(p.fdp, p.cvp, p.sps, p.svp, p.evp, p.gps)
umweg2 <- swissvotesDB1%>%dplyr::select(contains("w."))
shares <- umweg*umweg2
swissvotesDB1$proshares <- rowSums(shares)


pred_data <- readRDS("/Users/twilli/Dropbox/politan.ch/swissdd/prediction.Rds")%>%
  dplyr::select(bfsnr, welle_3_ja, welle_2_ja, trend)

swissvotesDB1 <- swissvotesDB1 %>%
  left_join(pred_data, "bfsnr")

strijbis <- readRDS("/Users/twilli/Dropbox/UniZH/phd/Projekte/Campaign_expenditure/data/campaign_expenditure_agg_final.Rds") %>% 
  dplyr::select(bfsnr, pm_pred)

swissvotesDB1 <- swissvotesDB1 %>%
  left_join(strijbis, "bfsnr")


library(caret)
library("rpart")
library("caretEnsemble")
library(randomForest)
library(nnet)
v <- list()
for(i in 1:100){
inTrain <- createDataPartition(y = swissvotesDB1$yesshare, p = .9, list = FALSE)
training <- swissvotesDB1[ inTrain,]
testing <- swissvotesDB1[-inTrain,]

#linear 
mod.lm <- lm(yesshare ~ nr_yesshare*rechtsform +sr_yesshare*rechtsform, data=training)
pred <- data.frame(bfsnr=training$bfsnr,
                   real=training$yesshare,
                   lm = predict(mod.lm, training))

#linear prediction strijbis
mod.lm.stri <- lm(yesshare ~ pm_pred, data=training)
stri <- training %>% dplyr::select(bfsnr, yesshare, pm_pred) %>% na.omit
mod2.lm.str <- lm(yesshare ~ pm_pred, data=stri)
stri$lm2 <- predict(mod2.lm.str, stri)
pred <- merge(pred, stri %>% dplyr::select(bfsnr, lm2), "bfsnr", all.x=T)

#linear welle 3 
welle3 <- training %>% dplyr::select(bfsnr, yesshare, welle_3_ja, trend, rechtsform) %>% na.omit
mod3.lm <- lm(yesshare ~ welle_3_ja+rechtsform*trend, data=welle3)
welle3$lm3 <- predict(mod3.lm, welle3)
pred <- merge(pred, welle3 %>% dplyr::select(bfsnr, lm3), "bfsnr", all.x=T)

#random forest
rf.training <- training %>% dplyr::select(bfsnr, yesshare, nr_yesshare, rechtsform) %>% na.omit
mod.rf <- randomForest(yesshare ~ nr_yesshare*rechtsform, data=rf.training, importance=TRUE,proximity=TRUE,mtry=10)
rf.training$rf <- predict(mod.rf, training)
pred <- merge(pred, rf.training %>% dplyr::select(bfsnr, rf), "bfsnr", all.x=T)

#neural net
nnet.training <- training %>% dplyr::select(bfsnr, yesshare, nr_yesshare, proshares, rechtsform) %>% na.omit
mod.nnet <- train(yesshare ~ proshares*rechtsform, nnet.training, method='nnet', linout=TRUE, trace = FALSE, tuneGrid=expand.grid(.size=c(1,5,10),.decay=c(0,0.001,0.1))) 
nnet.training$nnet <- predict(mod.nnet, training)
pred <- merge(pred, nnet.training %>% dplyr::select(bfsnr, nnet), "bfsnr", all.x=T)

#support vector machine
svm.training <- training %>% dplyr::select(bfsnr, yesshare, proshares, nr_yesshare, rechtsform) %>% na.omit
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
mod.svm <- train(yesshare ~ proshares+nr_yesshare*rechtsform, data = svm.training, method = "svmLinear", trControl = train_control,  preProcess = c("center","scale"))
svm.training$svm <- predict(mod.svm, training)
pred <- merge(pred, svm.training %>% dplyr::select(bfsnr, svm), "bfsnr", all.x=T)



#ebma 
library(EBMAforecast)

newd <- data.frame(bfsnr=testing$bfsnr,
                   real=testing$yesshare,
                   lm = predict(mod.lm, testing),
                   lm2 = predict(mod2.lm.str, testing),
                   rf = predict(mod.rf, testing),
                   svm= as.numeric(predict(mod.svm, testing)),
                   nnet = predict(mod.nnet, testing))

w3testing <- testing %>% dplyr::select(bfsnr, welle_3_ja,trend, rechtsform) %>% na.omit
w3testing$lm3 <- predict(mod3.lm, w3testing)
newd <- merge(newd, w3testing %>% dplyr::select(bfsnr, lm3), "bfsnr", all.x=T)


this.ForecastData <- makeForecastData(
  .predCalibration=pred[,c("lm", "lm2", "lm3", "rf", "nnet", "svm")],
  .outcomeCalibration=pred[,"real"],
                                    .predTest=newd[,c("lm", "lm2", "lm3", "rf", "nnet", "svm")], 
                                    .outcomeTest=newd[,c("real")], 
                                    .modelNames=c("lm", "lm2", "lm3", "rf", "nnet", "svm"))


thisEnsemble<-calibrateEnsemble(this.ForecastData, model="normal", useModelParams=TRUE, 
                                predType = "posteriorMedian", method="EM",
                                tol=0.000001)

pred$ensemble <- thisEnsemble@predCalibration[,,1][,"EBMA"]

mean(abs(pred$ensemble-pred$real))

newd$ensemble <- thisEnsemble@predTest[,,1][,"EBMA"]

v[[i]]<-mean(abs(newd$ensemble-newd$real), na.rm=T)
}

mean(unlist(v))

#paste togehther
all1 <- bind_rows(pred, newd)
all1$diff <- all1$real-all1$ensemble
mean(abs(all1$diff))



all <- all1 %>%
  tidyr::pivot_longer(-bfsnr, names_to = "algo", values_to = "values")

ggplot(all, aes(x=bfsnr, y=values, group=algo, color=algo))+
  geom_hline(yintercept=.5, size=.2)+
 # scale_color_manual(values=c("red", "grey70", "grey70", "grey70", "grey70", "green",  "grey70", "grey70"))+
  geom_point()







#combine preds
wts <- lm(real ~ lm + lm2 + lm3 +rf  + nnet + svm, pred)
coef(wts)

newd <- data.frame(lm = predict(mod.lm, testing),
                   rf = predict(mod.rf, testing),
                   svm= as.numeric(predict(mod.svm, testing)),
                   nnet = predict(mod.nnet, testing))

oos_pred <- predict(wts, newdata=newd)













oos <- data.frame(bfsnr=testing$bfsnr,
                   real=testing$yesshare,
                   stacking=oos_pred)%>%
  bind_cols(newd)%>%
  mutate(difference=real-stacking)

mean(abs(oos$difference))


wts2 <- lm(real ~ stacking + lm + rf  + nnet + svm, oos)
preds <- predict(wts2, oos)

oos2 <- data.frame(bfsnr=testing$bfsnr,
                   real=testing$yesshare,
                   all=preds)%>%
  mutate(difference=real-all)

mean(abs(oos2$difference))










my_control <- trainControl(
  method="repeatedcv",
  number=10,
  savePredictions="final",
  #classProbs=TRUE,
  index=createResample(training$yesshare, 25)
  )

library("mlbench")
library("randomForest")
library("nnet")


model_list <- caretList(
  yesshare~., data=training,
  trControl=my_control,
  methodList=c("glm", "rpart", "glmboost", "rf", "gam", "glmnet", "nnet", "svmLinear"),
  tuneList=list(
    rf=caretModelSpec(method="rf", tuneGrid=data.frame(.mtry=2))
  ))

glm_ensemble <- caretStack(
  model_list,
  method="lm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final"
  )
)



ensemble <- predict(glm_ensemble, newdata=testing)
inspect <- data.frame(bsnr=testing$bfsnr,
                      real=testing$yesshare,
                      predicted=ensemble)%>%
  mutate(difference = real-predicted)

mean(inspect$difference)


ggplot(inspect, aes(x=real, y=predicted))+
  geom_abline(intercept = 0, slope=1, size=.1)+
  geom_point()+
  scale_y_continuous(limits=c(0,1), labels=scales::percent)+
  scale_x_continuous(limits=c(0,1), labels=scales::percent)+
  geom_segment(aes(x=real, y=predicted, xend = real, yend = predicted+difference, colour = "segment"))+
  annotate("rect", xmin = 0.5, xmax = 1, ymin = 0, ymax = .5, fill="red", alpha = .2)+
  annotate("rect", xmin = 0, xmax = .5, ymin = 0.5, ymax = 1, fill="red", alpha = .2)






plot1 <- ggplot(swissvotesDB, aes(x=nr_yesshare, y= yesshare))+
  geom_hline(yintercept=.5, linetype="dashed", color="grey70")+
  geom_point()+
  geom_abline(intercept = 0, slope=1, size=.1)+
  scale_y_continuous(limits=c(0,1), labels=scales::percent)+
  scale_x_continuous(limits=c(0,1), labels=scales::percent)+
  geom_smooth(method="lm", se=T, size=.5, color="#FF6B00")+
  labs(y="Actual Outcome of Vote", x="Yes Share in National Council", caption="politan.ch")+
  theme_bw()
plot1
```

There seems to be a positive correlation (what a surprise!)... In a next step, we estimate a very basic model and then predict the vote shares for the two national votes on February 9th, 2020. In order to do so, we split the data into a training set and a prediction set.

```{r echo=TRUE, warning=FALSE, message=FALSE, eval=TRUE}
#split data
estimation_data <- swissvotesDB[!swissvotesDB$bfsnr%in%c(6290, 6300),]
prediction_data <- swissvotesDB[swissvotesDB$bfsnr%in%c(6290, 6300),]

#estimate a model
fit_m1 <- lm(yesshare ~ nr_yesshare, data=estimation_data)

#predict
prediction <- predict(fit_m1, newdata = prediction_data, interval="predict") %>% as.data.frame
prediction$bfsnr <- c(6290, 6300)
```

We can now plot the predictions for the two ballots. Note, it is crucial to correctly report uncertainty, hence, we have to include the prediction interval as well. 

```{r echo=TRUE, warning=FALSE, message=FALSE, eval=TRUE}

ggplot(prediction, aes(x=as.factor(bfsnr), y=fit, ymin=lwr, ymax=upr))+
  geom_pointrange(aes(color="#FF6B00"))+
  geom_text(aes(label=round(fit*100, 1)), nudge_x = 0.05)+
  scale_y_continuous(limits=c(0,1), labels=scales::percent)+
  scale_x_discrete(labels=c("Housing\nInitiative", "Law against\ndiscrimination"))+
  geom_hline(yintercept=.5, linetype="dashed", color="grey70")+
  labs(title="Simple Prediction of Vote-Shares", subtitle="February 9th, 2020", y="Predicted Yes-Share", x="", caption="politan.ch")+
  theme_bw()+
  theme(legend.position="none",
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_blank())

#add the points to the previous plot
prediction_data$yesshare[prediction_data$bfsnr==6290] <- prediction$fit[1]
prediction_data$yesshare[prediction_data$bfsnr==6300] <- prediction$fit[2]

prediction_data$lwr[prediction_data$bfsnr==6290] <- prediction$lwr[1]
prediction_data$lwr[prediction_data$bfsnr==6300] <- prediction$lwr[2]

prediction_data$upr[prediction_data$bfsnr==6290] <- prediction$upr[1]
prediction_data$upr[prediction_data$bfsnr==6300] <- prediction$upr[2]

plot1 + geom_point(data=prediction_data, aes(y=yesshare, x=nr_yesshare), color="#FF6B00", size=3)+
  geom_linerange(data=prediction_data,aes(ymin=lwr, ymax=upr), color="#FF6B00")

```









